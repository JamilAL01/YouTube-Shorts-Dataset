{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wagD4NyEmHYh"
      },
      "source": [
        "Define **API Keys** and a **Switching Function** the Keys **(GOAL: Prevent Quota Exsaution)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz84wzH2yeYH"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import googleapiclient.discovery\n",
        "import isodate\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "# YouTube API Keys Rotation\n",
        "API_KEYS = [\"YOUR_API_KEY\"]\n",
        "current_key = 0\n",
        "\n",
        "\n",
        "def switch_api_key(): #In case of using multiple keys! \"In our case, we use multiple keys with different accounts: 1 API Key per account\"\n",
        "    \"\"\"Switches API key when quota is exceeded.\"\"\"\n",
        "    global current_key, youtube\n",
        "    current_key = (current_key + 1) % len(API_KEYS)\n",
        "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEYS[current_key])\n",
        "\n",
        "# Initialize YouTube API\n",
        "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=API_KEYS[current_key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZTwiTOqySPi"
      },
      "outputs": [],
      "source": [
        "def search_youtube_shorts(query):\n",
        "    \"\"\"Search for YouTube Shorts videos based on a query.\"\"\"\n",
        "    max_results = 50\n",
        "    request = youtube.search().list(\n",
        "        part='snippet',\n",
        "        maxResults=max_results,\n",
        "        q=f\"{query} #shorts\", #Put \"shorts\" query to push the searching API to fetch only short videos.\n",
        "        type='video'\n",
        "    )\n",
        "    response = request.execute()\n",
        "    return response\n",
        "\n",
        "def get_video_durations(video_ids):\n",
        "    \"\"\"Get video durations and ensure they are 60s or less.\"\"\"\n",
        "    video_id_str = \",\".join(video_ids)\n",
        "    request = youtube.videos().list(part=\"contentDetails\", id=video_id_str)\n",
        "    response = request.execute()\n",
        "\n",
        "    durations = {}\n",
        "    for item in response.get(\"items\", []):\n",
        "        video_id = item[\"id\"]\n",
        "        duration_iso = item[\"contentDetails\"][\"duration\"]\n",
        "        duration_sec = isodate.parse_duration(duration_iso).total_seconds()\n",
        "\n",
        "        if duration_sec <= 60:\n",
        "            minutes, seconds = divmod(int(duration_sec), 60)\n",
        "            durations[video_id] = f\"{minutes}:{seconds:02d}\"\n",
        "\n",
        "    return durations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caBAvDekynSF"
      },
      "outputs": [],
      "source": [
        "def load_existing_video_ids(filename):\n",
        "    \"\"\"Load existing video IDs to prevent duplicates.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        return set()\n",
        "\n",
        "    existing_ids = set()\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        for row in reader:\n",
        "            existing_ids.add(row['short_url'].split('/')[-1])\n",
        "    return existing_ids\n",
        "\n",
        "# Save Data to CSV\n",
        "def save_to_csv(data, filename):\n",
        "    keys = data[0].keys()\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
        "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
        "        dict_writer.writeheader()\n",
        "        dict_writer.writerows(data)\n",
        "\n",
        "\n",
        "def save_raw_data(data, filename):\n",
        "    \"\"\"Append new data to JSON file while preventing duplicates.\"\"\"\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as json_file:\n",
        "            try:\n",
        "                existing_data = json.load(json_file)  # Load existing data\n",
        "            except json.JSONDecodeError:\n",
        "                existing_data = []  # Handle empty or corrupt JSON\n",
        "    else:\n",
        "        existing_data = []\n",
        "\n",
        "    # Convert existing video URLs into a set to avoid duplicates\n",
        "    existing_ids = {entry['short_url'] for entry in existing_data}\n",
        "\n",
        "    # Add only new videos that aren't already in the dataset\n",
        "    new_data = [video for video in data if video['short_url'] not in existing_ids]\n",
        "\n",
        "    if new_data:  # Save only if there are new videos\n",
        "        existing_data.extend(new_data)  # Append new videos\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as json_file:\n",
        "            json.dump(existing_data, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Added {len(new_data)} new videos to {filename}\")\n",
        "    else:\n",
        "        print(\"No new videos to add. Dataset is up-to-date.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELmOlQAGmV-n"
      },
      "outputs": [],
      "source": [
        "CATEGORY_MAPPING = { # Mapping between category-name and category-id\n",
        "    \"1\": \"Film & Animation\", \"2\": \"Cars & Vehicles\", \"10\": \"Music\",\n",
        "    \"15\": \"Pets & Animals\", \"17\": \"Sports\",\n",
        "    \"19\": \"Travel & Events\", \"20\": \"Gaming\", \"22\": \"People & Blogs\",\n",
        "    \"23\": \"Comedy\", \"24\": \"Entertainment\", \"25\": \"News & Politics\",\n",
        "    \"26\": \"How-to & Style\", \"27\": \"Education\", \"28\": \"Science & Technology\",\n",
        "    \"29\": \"Nonprofits & Activism\"\n",
        "}\n",
        "def get_video_details(video_ids):\n",
        "    \"\"\"Get full details of videos and filter only Shorts (0-60s preferred, max 180s).\"\"\"\n",
        "    video_id_str = \",\".join(video_ids)\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet,statistics,contentDetails\",\n",
        "        id=video_id_str\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    video_details = {}\n",
        "    for item in response.get(\"items\", []):\n",
        "        video_id = item[\"id\"]\n",
        "        snippet = item[\"snippet\"]\n",
        "        statistics = item.get(\"statistics\", {})\n",
        "        content_details = item[\"contentDetails\"]\n",
        "\n",
        "        # Extract and parse duration\n",
        "        duration_sec = isodate.parse_duration(content_details[\"duration\"]).total_seconds()\n",
        "\n",
        "        # Strict filter: Ensure Shorts duration (preferring 0-60s, max 180s)\n",
        "        if duration_sec > 180:  # Ignore >3 min and very short ones\n",
        "            continue\n",
        "\n",
        "        # Extract category ID & map to category name\n",
        "        category_id = snippet.get(\"categoryId\", \"Unknown\")\n",
        "        category_name = CATEGORY_MAPPING.get(category_id, \"Unknown\")\n",
        "\n",
        "        # Store video details\n",
        "        video_details[video_id] = {\n",
        "            'title': snippet.get(\"title\", \"\"),\n",
        "            'description': snippet.get(\"description\", \"\"),\n",
        "            'published_at': snippet.get(\"publishedAt\", \"\"),\n",
        "            'channel_id': snippet.get(\"channelId\", \"\"),\n",
        "            'channel_name': snippet.get(\"channelTitle\", \"\"),\n",
        "            'category_id': category_id,\n",
        "            'category_name': category_name,\n",
        "            'tags': \", \".join(snippet.get(\"tags\", [])) if \"tags\" in snippet else \"\",\n",
        "            'view_count': statistics.get(\"viewCount\", 0),\n",
        "            'like_count': statistics.get(\"likeCount\", 0),\n",
        "            'comment_count': statistics.get(\"commentCount\", 0),\n",
        "            'duration': duration_sec  # Store duration in seconds\n",
        "        }\n",
        "\n",
        "    return video_details\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dtdW0e6yypj",
        "outputId": "68ae7c2a-c606-42be-be24-c31bbdb891be"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "   # General TikTok Trending Keywords for YouTube Shorts\n",
        "    trending_keywords = [\n",
        "      \"#Shorts\", \"#YouTubeShorts\", \"#ViralShorts\", \"#Trending\", \"#funnyvideos\",\n",
        "      \"#lifestyle\", \"#football\", \"#sport\", \"#hightech\", \"#tourist\", \"#travel\",\n",
        "      \"#streetphotography\", \"#food\", \"#healthyrecipes\", \"#gaming\", \"#videogames\",\n",
        "      \"#art\", \"#craft\", \"#photography\", \"#easyrecipe\", \"#petitdejeuner\", \"#experiment\",\n",
        "      \"#savoir\", \"#creativejournaling\", \"#fashion\", \"#entertainment\", \"#comedy\",\n",
        "      \"#reaction\", \"#challenge\", \"#funny\", \"#fitness\", \"#motivation\", \"#education\",\n",
        "      \"#tech\", \"#science\", \"#music\", \"#dance\", \"#howto\", \"#DIY\", \"#hacks\", \"#makeup\",\n",
        "      \"#beauty\", \"#carreview\", \"#motorcycle\", \"#adventure\", \"#animals\", \"#nature\",\n",
        "      \"#movieclips\", \"#cartoons\", \"#shortfilm\", \"#vlog\", \"#behindthescenes\"\n",
        "      ]\n",
        "\n",
        "\n",
        "    dataset_file = 'categorized_youtube_shorts.csv'\n",
        "    dataset_file_json = 'categorized_youtube_shorts.json'\n",
        "\n",
        "    existing_ids = load_existing_video_ids(dataset_file)\n",
        "    dataset = []\n",
        "\n",
        "    for query in trending_keywords:\n",
        "      try:\n",
        "          response = search_youtube_shorts(query=query)\n",
        "      except Exception as e:\n",
        "          print(f\"API Error: {e}. Switching API key...\")\n",
        "          switch_api_key()\n",
        "          continue  # Retry with a new API key\n",
        "\n",
        "      video_data = []\n",
        "      for item in response.get('items', []):\n",
        "          video_id = item['id']['videoId']\n",
        "          if video_id in existing_ids:\n",
        "              continue  # Skip duplicates\n",
        "\n",
        "          snippet = item['snippet']\n",
        "          video_data.append({\n",
        "              'short_url': f\"https://www.youtube.com/shorts/{video_id}\",\n",
        "              'channel_name': snippet['channelTitle']\n",
        "          })\n",
        "          existing_ids.add(video_id)  # Add to seen list\n",
        "\n",
        "      if video_data:\n",
        "          video_ids = [vid['short_url'].split('/')[-1] for vid in video_data]\n",
        "          details = get_video_details(video_ids)\n",
        "\n",
        "          for video in video_data:\n",
        "              vid_id = video['short_url'].split('/')[-1]\n",
        "              if vid_id in details:\n",
        "                  video.update(details[vid_id])  # Merge details into video data\n",
        "                  dataset.append(video)\n",
        "\n",
        "      time.sleep(random.uniform(1, 3))  # Prevent hitting rate limits\n",
        "\n",
        "    save_to_csv(dataset, dataset_file)\n",
        "    save_raw_data(dataset, dataset_file_json)\n",
        "    print(\"Dataset updated successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
